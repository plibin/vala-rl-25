{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DQN vs PPO: Practical Implementations**\n",
    "***Authors: Bram Silue, Prof. dr. Pieter Libin, Prof. dr. Niel Hens.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How to Run**\n",
    "Please open and run this notebook in Colab:\n",
    "\n",
    "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/plibin/vala-rl-25/blob/main/src/3_dqn_vs_ppo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "Previously, we developed a Deep Q-Network (DQN) from scratch to understand the foundational aspects of reinforcement learning algorithms. In practice, however, developers often leverage well-optimized, **pre-built libraries** instead. \n",
    "\n",
    "In this notebook, we will use the Stable Baselines library to implement two of the most significant reinforcement learning algorithms: Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). We will train these algorithms on the Lunar Lander environment from Gymnasium. The implementation we will follow is based on the Stable Baselines [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### **Implementing DQN**\n",
    "Stable-baselines allows for very straightforward usage of advanced reinforcemente learning algorithms. Let's start with DQN.\n",
    "\n",
    "First, we import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment.\n",
    "!pip install swig > /dev/null 2>&1\n",
    "!pip install gymnasium > /dev/null 2>&1\n",
    "!pip install \"gymnasium[box2d]\" > /dev/null 2>&1\n",
    "import gymnasium as gym\n",
    "\n",
    "# Machine Learning.\n",
    "!pip install stable-baselines3 > /dev/null 2>&1\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# Plotting.\n",
    "!pip install renderlab > /dev/null 2>&1\n",
    "import renderlab\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Numerical computing.\n",
    "import numpy as np\n",
    "\n",
    "# Other.\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the environment and set the random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment.\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "# Set the random seed.\n",
    "seed = 42\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model with specified parameters.\n",
    "model = DQN(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    batch_size=128,\n",
    "    buffer_size=50_000,\n",
    "    exploration_final_eps=0.1,\n",
    "    exploration_fraction=0.12,\n",
    "    gamma=0.99,\n",
    "    gradient_steps=-1,\n",
    "    learning_rate=0.00063,\n",
    "    learning_starts=0,\n",
    "    target_update_interval=250,\n",
    "    train_freq=4,\n",
    "    seed=seed,\n",
    "    policy_kwargs=dict(net_arch=[256, 256])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training the agent, we write a Callback object that will retrieve the rewards during training so that we can plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogAndSave_Callback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback for stable_baselines3 to log cumulative rewards per episode\n",
    "    and save the best model so far.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    save_path :  The file path to save the best model.\n",
    "    verbose   :  The verbosity level; 0 means silent, higher values increase verbosity.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 save_path: str, \n",
    "                 verbose: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the RewardLoggerCallback instance.\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        verbose :  Sets the verbosity level; defaults to 0 (silent).\n",
    "\n",
    "        \"\"\"\n",
    "        super(LogAndSave_Callback, self).__init__(verbose)\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Initialize reward logging\n",
    "        self.all_episode_rewards = []     # list of cumulative rewards, for all episodes.\n",
    "        self.episode_rewards = 0          # cumulative reward a single episode, initialized at 0.\n",
    "        self.best_rewards = -float('inf')  # the best reward seen so far, initialized at -inf.\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Executed at each step of the episode to log rewards and reset counters when an episode ends.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Always returns True to continue training.\n",
    "\n",
    "        \"\"\"\n",
    "        # Accumulate the rewards.\n",
    "        self.episode_rewards += self.locals['rewards'].item()\n",
    "\n",
    "        if self.locals['dones']:\n",
    "            # Log the episode's total reward.\n",
    "            self.all_episode_rewards.append(self.episode_rewards)\n",
    "            \n",
    "            # Save the model if it performs well.\n",
    "            if self.episode_rewards > self.best_rewards:\n",
    "                self.model.save(self.save_path)\n",
    "                self.best_rewards = self.episode_rewards\n",
    "\n",
    "            # Reset accumulated rewards for the next episode.\n",
    "            self.episode_rewards = 0\n",
    "\n",
    "        # Return True to continue training.\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we train the DQN agent we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the 'trained' directory exists.\n",
    "os.makedirs('trained', exist_ok=True)\n",
    "\n",
    "# Set the arguments.\n",
    "timesteps = 150_000\n",
    "callback = LogAndSave_Callback(save_path='trained/dqn.pth')\n",
    "\n",
    "# Train the model.\n",
    "model.learn(total_timesteps=timesteps, \n",
    "            callback=callback,\n",
    "            progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the trained DQN model and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = DQN.load('trained/dqn.pth', env=env)\n",
    "\n",
    "# Evaluate the model.\n",
    "mean_reward, std_reward = evaluate_policy(model=model, \n",
    "                                          env=model.get_env(), \n",
    "                                          n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward:.2f} ± {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also write a plotting function and plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model: DQN | PPO,\n",
    "                        rewards: list, \n",
    "                        window_length: int = 15, \n",
    "                        polyorder: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Plots a smoothed learning curve for cumulative rewards from a list.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    rewards       :  The list of cumulative rewards per episode.\n",
    "    window_length :  The length of the filter window.\n",
    "    polyorder     :  The order of the polynomial used to fit the samples.\n",
    "\n",
    "    \"\"\"\n",
    "    # Convert the rewards list to a numpy array for easier manipulation.\n",
    "    rewards_array = np.array(rewards)\n",
    "\n",
    "    # Apply Savitzky-Golay filter to smooth the rewards.\n",
    "    if len(rewards_array) >= window_length:\n",
    "        smoothed_rewards = savgol_filter(rewards_array, \n",
    "                                         window_length=window_length, \n",
    "                                         polyorder=polyorder)\n",
    "    else:\n",
    "        # If the data is less than the window length, do not smooth.\n",
    "        smoothed_rewards = rewards_array\n",
    "\n",
    "    # Create a new figure for the plot.\n",
    "    plt.figure()\n",
    "\n",
    "    # Plot the smoothed cumulative rewards.\n",
    "    plt.plot(smoothed_rewards, label='Smoothed Cumulative Reward', \n",
    "             linewidth=1, color='blue')\n",
    "\n",
    "    # Configure plot labels and legend.\n",
    "    plt.xlabel('Episode', fontweight='bold')\n",
    "    plt.ylabel('Cumulative Reward', fontweight='bold')\n",
    "    plt.title(f'{model.__class__.__name__}: Smoothed Learning Curve', \n",
    "              fontweight='bold')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model, rewards=callback.all_episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the policy of our trained DQN model, for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_policy(model: DQN | PPO, env: gym.Env, seed = 42) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes the policy of a given model via a 2D animation.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model :  The model (DQN or PPO).\n",
    "    emv   :  The environment.\n",
    "    seed  :  The random seed.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Wrap the environment using renderlab to make a video later.\n",
    "    env = renderlab.RenderFrame(env, \"./output\")\n",
    "\n",
    "    # Reset the environment, retrieve the state, and set the random seed.\n",
    "    state, _ = env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    env.observation_space.seed(seed)\n",
    "\n",
    "    # Simulate.\n",
    "    while True:\n",
    "      action, _ = model.predict(state, deterministic=True)   # compute the model's action.\n",
    "      state, _, terminated, truncated, _ = env.step(action)  # perform the action.\n",
    "      \n",
    "      if terminated or truncated:\n",
    "          break\n",
    "\n",
    "    # Play the video.\n",
    "    env.play()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(model, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Implementing PPO**\n",
    "\n",
    "We will now use the PPO algorithm from the Stable Baselines library. The approach is analogous to what we did earlier with DQN.\n",
    "\n",
    "First, we instantiate the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment.\n",
    "env = gym.make('LunarLander-v3', render_mode='rgb_array')\n",
    "\n",
    "# Set the random seed.\n",
    "env.reset(seed=seed)\n",
    "env.action_space.seed(seed)\n",
    "env.observation_space.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the PPO agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\n",
    "    policy='MlpPolicy',\n",
    "    env=env,\n",
    "    n_steps=512,\n",
    "    batch_size=128,\n",
    "    n_epochs=20,\n",
    "    learning_rate=7.5e-4,\n",
    "    gamma=0.999,\n",
    "    gae_lambda=0.9,\n",
    "    clip_range=0.2,\n",
    "    ent_coef=0.01,\n",
    "    vf_coef=0.4,\n",
    "    max_grad_norm=0.7,\n",
    "    policy_kwargs=dict(net_arch=[256, 256]),\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the PPO agent we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the arguments.\n",
    "timesteps = 150_000\n",
    "callback = LogAndSave_Callback(save_path='trained/ppo.pth')\n",
    "\n",
    "# Train the model.\n",
    "model.learn(total_timesteps=timesteps, \n",
    "            callback=callback,\n",
    "            progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now evaluate the PPO agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = PPO.load('trained/ppo.pth', env=env)\n",
    "\n",
    "# Evaluate the model.\n",
    "mean_reward, std_reward = evaluate_policy(model=model, \n",
    "                                          env=model.get_env(), \n",
    "                                          n_eval_episodes=10)\n",
    "print(f'Mean reward: {mean_reward:.2f} ± {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(model, rewards=callback.all_episode_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can visualize the policy of our trained PPO agent, for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_policy(model, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "espidam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
