{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4m460URQ6W5J"
      },
      "source": [
        "# **Getting Started with DQN**\n",
        "***Authors: Bram Silue, Prof. dr. Pieter Libin, Prof. dr. Niel Hens.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **How to Run**\n",
        "Please open and run this notebook in Colab:\n",
        "\n",
        "[![colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/plibin/vala-rl-25/blob/main/src/1_dqn.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sByKHRKr6W5O"
      },
      "source": [
        "### **Introduction**\n",
        "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by performing actions in an environment to maximize their cumulative reward. The **Deep Q-Network (DQN)** is a powerful learning RL algorithm that combines Q-learning with deep neural networks, enabling agents to learn effective policies in complex environments.\n",
        "\n",
        "In this notebook, we will explore how to implement a DQN agent and train it to navigate the [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/) environment from the OpenAI Gym. In the Lunar Lander simulation, the goal is to safely land a spacecraft between two flags on the lunar surface by controlling its thrusters.\n",
        "\n",
        "Here are some technical details about the Lunar Lander environment:\n",
        "- The state is an 8-dimensional vector, composed of:\n",
        "    - The coordinates of the spacecraft in x & y,\n",
        "    - The linear velocities in x & y of the spacecraft,\n",
        "    - The angle of the spacecraft,\n",
        "    - The angular velocity of the spacecraft,\n",
        "    - Two booleans that represent whether each leg is in contact with the ground or not.\n",
        "- The action space is composed of four discrete actions:\n",
        "    - Do nothing,\n",
        "    - Fire bottom thruster,\n",
        "    - Fire left thruster,\n",
        "    - Fire right thruster.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BHWvMOM6W5O"
      },
      "source": [
        "### **Libraries**\n",
        "We install and import the necessary libraries (this can take a while)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6pnmyA06W5P",
        "outputId": "8fd7b80c-66f9-4ed2-fc37-c51a423f1a3a"
      },
      "outputs": [],
      "source": [
        "# Environment.\n",
        "!pip install swig > /dev/null 2>&1\n",
        "!pip install gymnasium > /dev/null 2>&1\n",
        "!pip install \"gymnasium[box2d]\" > /dev/null 2>&1\n",
        "import gymnasium as gym\n",
        "\n",
        "# Plotting.\n",
        "!pip install renderlab > /dev/null 2>&1\n",
        "import renderlab\n",
        "\n",
        "# Machine learning.\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "print('\\n Device:', device)\n",
        "\n",
        "# Other tools.\n",
        "import csv\n",
        "import random\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IeMAnlv6W5Q"
      },
      "source": [
        "### **Reproducibility**\n",
        "We make sure to define the *trajectory seed*. The trajectory seed refers to the random seed used to generate the sequence of actions and states experienced by an agent. Trajectory seeds are crucial for reproducibility because they ensure that the same sequence of experiences is produced across different runs, enabling researchers to replicate experiments and compare results accurately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wtmKv71P6W5Q"
      },
      "outputs": [],
      "source": [
        "seed = 42  # choose any seed you like.\n",
        "if seed:\n",
        "    random.seed(seed)        # we set the random seed of Python's random number generator.\n",
        "    torch.manual_seed(seed)  # we set the random seed of PyTorch's random number generator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rlvs14F6var"
      },
      "source": [
        "### **Helper Functions**\n",
        "We define some helper functions that will make the code easier to read and understand later on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VXZ7cG1_65rj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "from scipy.signal import savgol_filter\n",
        "from IPython import display\n",
        "\n",
        "# Helper object for environment transitions.\n",
        "Transition = namedtuple('Transition', ('state',\n",
        "                                       'action',\n",
        "                                       'next_state',\n",
        "                                       'reward'))\n",
        "\n",
        "# Helper function for plotting.\n",
        "def plot_performance(cumulative_rewards: list[float],\n",
        "                     show_result: bool = False,\n",
        "                     window_length: int = 25,\n",
        "                     polyorder: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Plots the learning curve, i.e., performance during training.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    cumulative_rewards :  The list of cumulative rewards from episodes.\n",
        "    show_result        :  Flag to either show the final result or the ongoing training plot.\n",
        "\n",
        "    \"\"\"\n",
        "    # Create a new figure.\n",
        "    plt.figure(1)\n",
        "\n",
        "    # Convert the reward list to a tensor.\n",
        "    rewards_t = torch.tensor(cumulative_rewards, dtype=torch.float)\n",
        "\n",
        "    # Configure plot title based on whether or not we are still training.\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.title('Training...')\n",
        "\n",
        "    # Plot cumulative rewards.\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Cumulative Reward')\n",
        "    plt.plot(rewards_t.numpy(), label='Cumulative Reward', alpha=0.25)\n",
        "\n",
        "    # Apply Savitzky-Golay filter to smooth the curve.\n",
        "    # Smoothing is useful because it helps in identifying trends and patterns\n",
        "    # that might be obscured by noise and fluctuations in the raw results.\n",
        "    # (https://en.wikipedia.org/wiki/Savitzkyâ€“Golay_filter)\n",
        "    if len(rewards_t) >= window_length:\n",
        "        smoothed_rewards = savgol_filter(rewards_t.numpy(),\n",
        "                                         window_length=window_length,\n",
        "                                         polyorder=polyorder)\n",
        "        plt.plot(smoothed_rewards, label='Smoothed Cumulative Reward', linewidth=2, color='orange')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.pause(0.001)  # pause briefly to update plots.\n",
        "\n",
        "    # Display the plot in the notebook.\n",
        "    if not show_result:\n",
        "        display.display(plt.gcf())\n",
        "        display.clear_output(wait=True)\n",
        "    else:\n",
        "        display.display(plt.gcf())\n",
        "        plt.ioff()\n",
        "        plt.show()\n",
        "\n",
        "# Helper function for calculating epsilon.\n",
        "def calculate_epsilon(EPS_START: float,\n",
        "                      EPS_END: float,\n",
        "                      EPS_DECAY: float,\n",
        "                      steps_done: int) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the current value of epsilon.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    EPS_START  :  The starting value of epsilon.\n",
        "    EPS_END    :  The final value of epsilon.\n",
        "    EPS_DECAY  :  The decay parameter for epsilon.\n",
        "    steps_done :  The number of steps done so far.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    epsilon :  The current value of epsilon.\n",
        "\n",
        "    \"\"\"\n",
        "    epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
        "        math.exp(-1. * steps_done / EPS_DECAY)\n",
        "    return epsilon\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TNGwv036W5Q"
      },
      "source": [
        "### **Environment Setup**\n",
        "We set up the environment and visualize it. First, we define a function that visualizes how an agent acts in a given environment. Our visualization will run 3 episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CR-4FH056W5Q"
      },
      "outputs": [],
      "source": [
        "def simulate_agent(env: gym.Env,\n",
        "                   policy_net: 'DQN' = None) -> None:\n",
        "    \"\"\"\n",
        "    This function simulates an agent acting in a given environment.\n",
        "    The agent will act according to the provided policy network if given,\n",
        "    otherwise it will act randomly.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    env        :  The environment.\n",
        "    policy_net :  The policy network (optional). If provided, the agent\n",
        "                  will use this network to select actions.\n",
        "                  \n",
        "    \"\"\"\n",
        "    # Reset the environment, retrieve the state, and set the random seed.\n",
        "    state, _ = env.reset(seed=seed)\n",
        "    env.action_space.seed(seed)\n",
        "    env.observation_space.seed(seed)\n",
        "\n",
        "    # Convert the state object to a tensor. This is needed because if we\n",
        "    # pass this state to a policy, the policy expects the state in\n",
        "    # tensor form for fast neural network computations.\n",
        "    state = torch.tensor(state,\n",
        "                         device=device).unsqueeze(0)  # `unsqueeze(0) turns the single observation\n",
        "                                                      # into a minibatch containing one observation.\n",
        "\n",
        "    # Simulate.\n",
        "    while True:\n",
        "\n",
        "        # Select action.\n",
        "        # If a policy is passed as urgment, we use it.\n",
        "        if policy_net:\n",
        "            with torch.no_grad():\n",
        "                # Pass the state through the policy network to get the output.\n",
        "                # The output is the Q-values for each action.\n",
        "                output = policy_net(state)\n",
        "\n",
        "                # Find the maximum Q-value and its corresponding action index\n",
        "                # max(1) returns a tuple (values, indices) with:\n",
        "                # - values: the maximum Q-value for each batch element.\n",
        "                # - indices: the index of the maximum Q-value (i.e., the action)\n",
        "                #            for each minibatch element.\n",
        "                max_q_value, max_q_index = output.max(1)\n",
        "\n",
        "                # Reshape the index tensor to be of shape [1, 1], i.e.,\n",
        "                # a minibatch of size 1 with a single action inside.\n",
        "                max_q_index = max_q_index.view(1, 1)\n",
        "\n",
        "                # Convert the index tensor to a Python integer,\n",
        "                # which is the selected action.\n",
        "                action = max_q_index.item()\n",
        "\n",
        "        # If no policy is passed, act randomly.\n",
        "        else:\n",
        "            # Select (i.e., sample) a random action.\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        # Perform action.\n",
        "        observation, _, terminated, truncated, _ = env.step(action)\n",
        "\n",
        "        # Check if the episode is done.\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        else:\n",
        "            # Update the current state and convert to a tensor.\n",
        "            state = torch.tensor(observation,\n",
        "                                 device=device).unsqueeze(0)\n",
        "\n",
        "    # Play the video.\n",
        "    env.play()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioLIDQx-6W5R"
      },
      "source": [
        "Next, we create the environment and run the simulation. A render window will pop up, showing 3 episodes where the spacecraft likely crashes on the ground. This is expected, because the agent acts randomly and has not learned to pilot the spacecraft. Do not close the window when done, we will use it again later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "2ipoAY7h6W5R",
        "outputId": "b1c97b3d-d18f-44e5-d141-723b7b125c93"
      },
      "outputs": [],
      "source": [
        "# Create environment.\n",
        "env_name = 'LunarLander-v3'\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "\n",
        "# Wrap the environment using renderlab to make a video later.\n",
        "env = renderlab.RenderFrame(env, \"./output\")\n",
        "\n",
        "# Simulate a random agent.\n",
        "simulate_agent(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61gt5kVd6W5R"
      },
      "source": [
        "### **Implementing DQN**\n",
        "In the simulation we visualized above, the agent performs random actions and likely fails to land the spacecraft. We now implement a DQN agent that will later be trained to learn how to land the spacecraft. This DQN implementation is inspired by the Pytorch [tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) on DQN.\n",
        "\n",
        "First, we create the replay memory and deep q-network classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOSKKXnO6W5R"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory:\n",
        "    \"\"\"\n",
        "    The replay memory, to store transitions for experience replay.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    capacity :  The maximum number of transitions to store in memory.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, capacity: int) -> None:\n",
        "        # We implement the memory using a deque data structure, which is\n",
        "        # a double-ended queue implementation from Python's standard library.\n",
        "        # We use a deque because the addition and removal of elements\n",
        "        # at the front or back of a deque happens with a time complexity\n",
        "        # of O(1), which is highly efficient.\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, transition: Transition) -> None:\n",
        "        \"\"\"\n",
        "        Save a transition.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        *args :  The state, action, next_state, and reward to be stored.\n",
        "\n",
        "        \"\"\"\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size: int) -> list[Transition]:\n",
        "        \"\"\"\n",
        "        Samples a minibatch of transitions.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        batch_size :  The number of transitions to sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        A minibatch (list) of sampled transitions.\n",
        "\n",
        "        \"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the current size of the internal memory.\n",
        "\n",
        "        \"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-LVo3ds6W5R"
      },
      "outputs": [],
      "source": [
        "class DQN(nn.Module):\n",
        "    \"\"\"\n",
        "    The Deep Q-Network.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    state_dim  :  The number of input features (observations).\n",
        "    action_dim :  The number of output actions.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, state_dim: int, action_dim: int) -> None:\n",
        "        super(DQN, self).__init__()\n",
        "        # Create neural network layers.\n",
        "        # The expression `nn.Linear(X,Y)` creates a fully connected layer\n",
        "        # that performs a linear transformation on the input of size X\n",
        "        # and returns an output of size Y.\n",
        "        self.layer1 = nn.Linear(state_dim, 256)\n",
        "        self.layer2 = nn.Linear(256, 256)\n",
        "        self.layer3 = nn.Linear(256, action_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass through the network.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        x :  The input tensor.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        The output tensor.\n",
        "\n",
        "        \"\"\"\n",
        "        # Pass the input through the first fully connected layer, then\n",
        "        # apply the ReLU activation function to introduce non-linearity.\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG6Xr8Pe6W5S"
      },
      "source": [
        "Then, we create the DQN Agent class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "libmUnpJ6W5S"
      },
      "outputs": [],
      "source": [
        "class DQN_Agent:\n",
        "    \"\"\"\n",
        "    The DQN Agent for training and interacting with the environment.\n",
        "\n",
        "    Arguments\n",
        "    ---------\n",
        "    env          :  The environment in which the agent will interact.\n",
        "    BUFFER_SIZE  :  The size of the replay buffer.\n",
        "    BATCH_SIZE   :  The size of the minibatch used for training.\n",
        "    GAMMA        :  The discount factor for future rewards.\n",
        "    EPS_START    :  The starting value of epsilon.\n",
        "    EPS_END      :  The minimum value of epsilon after decay.\n",
        "    EPS_DECAY    :  The decay parameter for epsilon.\n",
        "    TAU          :  The factor for soft updating the target network.\n",
        "    LR           :  The learning rate for the optimizer.\n",
        "    UPDATE_EVERY :  How often (in steps) to update the model\n",
        "    seed         :  The seed for reproducibility.\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, env: gym.Env,\n",
        "                 BUFFER_SIZE: int = 10_000,\n",
        "                 BATCH_SIZE: int = 64,\n",
        "                 GAMMA: float = 0.99,\n",
        "                 EPS_START: float = 0.99,\n",
        "                 EPS_END: float = 0.00,\n",
        "                 EPS_DECAY: float = 1000,\n",
        "                 TAU: float = 0.005,\n",
        "                 LR: float = 5e-4,\n",
        "                 UPDATE_EVERY: int = 2,\n",
        "                 seed: int = 42,\n",
        "                 save: bool = False) -> None:\n",
        "\n",
        "        self.env = env\n",
        "\n",
        "        # Set parameters.\n",
        "        self.BUFFER_SIZE = BUFFER_SIZE\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.GAMMA = GAMMA\n",
        "        self.EPS_START = EPS_START\n",
        "        self.EPS_END = EPS_END\n",
        "        self.EPS_DECAY = EPS_DECAY\n",
        "        self.TAU = TAU\n",
        "        self.LR = LR\n",
        "        self.UPDATE_EVERY = UPDATE_EVERY\n",
        "        self.seed = seed\n",
        "        self.save = save\n",
        "\n",
        "        # Set the environment's random seed.\n",
        "        state, _ = self.env.reset(seed=seed)\n",
        "        self.env.action_space.seed(seed)\n",
        "        self.env.observation_space.seed(seed)\n",
        "\n",
        "        # Get action and state dimensions.\n",
        "        self.action_dim = self.env.action_space.n\n",
        "        self.state_dim = len(state)\n",
        "\n",
        "        # Initialize model components, which are:\n",
        "        # - two networks: the policy network, and the target network.\n",
        "        # - the replay memory.\n",
        "        self.policy_net = DQN(self.state_dim, self.action_dim).to(device)\n",
        "        self.target_net = DQN(self.state_dim, self.action_dim).to(device)\n",
        "        self.memory = ReplayMemory(BUFFER_SIZE)\n",
        "        # Copy the weights (parameters) from the policy network to the target network.\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        # Initialize the optimizer. We use an Adam optimizer, which is a\n",
        "        # stochastic gradient descent (SGD) optimization algorithm that\n",
        "        # uses an adaptive learning rate based on estimates of the\n",
        "        # first and second moments.\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(),\n",
        "                                     lr=LR, amsgrad=True)\n",
        "\n",
        "    def train(self, num_episodes: int) -> None:\n",
        "        \"\"\"\n",
        "        Trains the model over a specified number of episodes.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        num_episodes :  The number of episodes to train the model.\n",
        "\n",
        "        \"\"\"\n",
        "        # Start interactive plotting.\n",
        "        plt.ion()\n",
        "\n",
        "        # Initialize tracking variables.\n",
        "        steps_done = 0\n",
        "        cumulative_rewards = []\n",
        "\n",
        "        for i in range(num_episodes):\n",
        "            cumulative_reward = 0\n",
        "            state, _ = self.env.reset(seed=self.seed)\n",
        "            self.env.action_space.seed(seed)\n",
        "            self.env.observation_space.seed(seed)\n",
        "            state = torch.tensor(state, device=device).unsqueeze(0)\n",
        "\n",
        "            # Perform an episode.\n",
        "            while True:\n",
        "                action = self.select_action(state, steps_done)\n",
        "                steps_done += 1\n",
        "\n",
        "                # Take the action and observe the new state and reward.\n",
        "                observation, reward, terminated, truncated, _ = self.env.step(action.item())\n",
        "                reward = torch.tensor([reward], device=device)\n",
        "                cumulative_reward += reward.item()\n",
        "\n",
        "                # Check for the end of the episode.\n",
        "                done = terminated or truncated\n",
        "                if terminated:\n",
        "                    next_state = None\n",
        "                else:\n",
        "                    next_state = torch.tensor(observation, device=device).unsqueeze(0)\n",
        "\n",
        "                # Store the transition in replay memory.\n",
        "                transition = Transition(state, action, next_state, reward)\n",
        "                self.memory.push(transition)\n",
        "\n",
        "                # Update the current state.\n",
        "                state = next_state\n",
        "\n",
        "                # Perform the model optimization step.\n",
        "                if steps_done % self.UPDATE_EVERY == 0:\n",
        "                    self.do_optimization_step()\n",
        "\n",
        "                    # Soft update the target network. # First, we get the current state\n",
        "                    # (parameters) of the target network as a dictionary.\n",
        "                    target_net_state_dict = self.target_net.state_dict()\n",
        "\n",
        "                    # Next, we get the current state (parameters) of the policy network.\n",
        "                    # as a dictionary.\n",
        "                    policy_net_state_dict = self.policy_net.state_dict()\n",
        "\n",
        "                    # Next, we iterate over each parameter in the policy network's state dictionary.\n",
        "                    for key in policy_net_state_dict:\n",
        "                        # We perform a soft update of the target network's parameters.\n",
        "                        # The new parameter value for the target network is a weighted sum of:\n",
        "                        # - The corresponding parameter value from the policy network (weighted by TAU).\n",
        "                        # - The existing parameter value from the target network (weighted by (1 - TAU)).\n",
        "                        target_net_state_dict[key] = policy_net_state_dict[key] * self.TAU \\\n",
        "                                                   + target_net_state_dict[key] * (1 - self.TAU)\n",
        "                    # Finally, we load the updated state dictionary back into the target network.\n",
        "                    self.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "\n",
        "                # Break out of the loop if the episode is done.\n",
        "                if done:\n",
        "                    cumulative_rewards.append(cumulative_reward)\n",
        "                    plot_performance(cumulative_rewards)\n",
        "                    break\n",
        "\n",
        "        # When training is done, show the final plot.\n",
        "        plot_performance(cumulative_rewards, show_result=True)\n",
        "\n",
        "        # Save learning curve to a CSV file.\n",
        "        if self.save:\n",
        "            save_path = 'learning_curves/learning_curve__seed=' \\\n",
        "                      + str(self.seed) + '.csv'\n",
        "            with open(save_path, 'w', newline='') as file:\n",
        "                writer = csv.writer(file)\n",
        "                writer.writerow(['Episode', 'Cumulative Reward'])\n",
        "                for episode, reward in enumerate(cumulative_rewards):\n",
        "                    writer.writerow([episode + 1, reward])\n",
        "\n",
        "    def select_action(self,\n",
        "                      state: torch.Tensor,\n",
        "                      steps_done: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Select an action based on an epsilon-greedy policy.\n",
        "\n",
        "        Arguments\n",
        "        ---------\n",
        "        state      :  The current state.\n",
        "        steps_done :  The number of steps done.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        action :  The selected action.\n",
        "\n",
        "        \"\"\"\n",
        "        # Given the parameter values for the start, end, and decay\n",
        "        # of epsilon, and given the amount of steps done so far,\n",
        "        # we can calculate the current value of epsilon. We employ\n",
        "        # a helper function from `utils/helpers.py` that uses the\n",
        "        # following formula:\n",
        "        #\n",
        "        #  epsilon = EPS_END + (EPS_START - EPS_END)\n",
        "        #          * math.exp(-1. * steps_done / EPS_DECAY)\n",
        "        #\n",
        "        epsilon = calculate_epsilon(self.EPS_START, self.EPS_END,\n",
        "                                    self.EPS_DECAY, steps_done)\n",
        "\n",
        "        # With probability epsilon, select a random action.\n",
        "        if random.random() < epsilon:\n",
        "            action = torch.tensor(\n",
        "                [[self.env.action_space.sample()]],\n",
        "                device=device,\n",
        "                dtype=torch.long\n",
        "            )\n",
        "        # Otherwise, select the best action learned so far.\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                # Pass the state through the policy network to get the output.\n",
        "                # The output is the Q-values for each action.\n",
        "                output = self.policy_net(state)\n",
        "\n",
        "                # Find the maximum Q-value and its corresponding action index\n",
        "                # max(1) returns a tuple (values, indices) with:\n",
        "                # - values: the maximum Q-value for each minibatch element.\n",
        "                # - indices: the index of the max Q-value (i.e., the action) for each minibatch element.\n",
        "                max_q_value, max_q_index = output.max(1)\n",
        "\n",
        "                # Reshape the index tensor to be of shape [1, 1]. The result is\n",
        "                # a minibatch of size 1 containing a single action.\n",
        "                action = max_q_index.view(1, 1)\n",
        "\n",
        "        return action\n",
        "\n",
        "    def do_optimization_step(self) -> None:\n",
        "        \"\"\"\n",
        "        Performs an optimization step of the model by sampling a minibatch from the\n",
        "        replay memory and updating the policy network based on the loss.\n",
        "\n",
        "        \"\"\"\n",
        "        # Only proceed if we have enough samples in memory.\n",
        "        if len(self.memory) < self.BATCH_SIZE:\n",
        "            return\n",
        "\n",
        "        # Sample a minibatch of transitions from memory.\n",
        "        transitions = self.memory.sample(self.BATCH_SIZE)\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        # Separate the sampled transitions into different batches.\n",
        "        non_final_mask = torch.tensor(\n",
        "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
        "            device=device, dtype=torch.bool\n",
        "        )\n",
        "        non_final_next_states = torch.cat(\n",
        "            [s for s in batch.next_state if s is not None]\n",
        "        )\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        # Compute Q(s_t, a).\n",
        "        # Recall that the policy network (self.policy_net) computes Q-values for all actions\n",
        "        # given the current minibatch of states (state_batch). We use the .gather() method to\n",
        "        # select the Q-values corresponding to the specific actions taken (action_batch).\n",
        "        # In other words, we get Q(s_t, a) for the actions that were taken in those states.\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        # Compute V(s_{t+1}) for all next states.\n",
        "        # We first initialize the tensor that will store the V(s_{t+1}) values, by filling it\n",
        "        # with zeros for now.\n",
        "        next_state_values = torch.zeros(self.BATCH_SIZE, device=device)\n",
        "        # Next, we use `torch.no_grad()` to ensure no gradients are calculated for the computations\n",
        "        # we are about to do, as we do not want to backpropagate them through the target network.\n",
        "        with torch.no_grad():\n",
        "            # For non-final next states, we compute the maximum Q-value predicted by the target network.\n",
        "            # We consider non-final next states because for final states, the next state does not exist.\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n",
        "        # We now compute the expected Q-values for the actions taken, combining immediate rewards and\n",
        "        # the discounted maximum future rewards. This is done for all transitions in the batch.\n",
        "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
        "\n",
        "        # Compute the loss.\n",
        "        # To compute it, we use the SmoothL1Loss criterion, which uses a squared term\n",
        "        # if the absolute element-wise error falls below beta and an L1 term otherwise.\n",
        "        # It is less sensitive to outliers than torch.nn.MSELoss and in some cases\n",
        "        # prevents gradients from exploding (becoming exceedingly large).\n",
        "        criterion = nn.SmoothL1Loss()\n",
        "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        # Optimize the model.\n",
        "        # First, we set the gradients of all the model parameters to zero. This\n",
        "        # is important because gradients are accumulated by default in PyTorch.\n",
        "        self.optimizer.zero_grad()\n",
        "        # Then, we backpropagate the loss through the network to compute the\n",
        "        # gradients of the loss with respect to the model parameters.\n",
        "        loss.backward()\n",
        "        # We clip the gradients to a maximum value of 100. Gradient clipping is used to\n",
        "        # prevent the gradients from exploding during training.\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        # Finally, we update the model parameters using the gradients. This step applies\n",
        "        # the calculated gradients to the model parameters to minimize the loss.\n",
        "        self.optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFfqdsVI6W5S"
      },
      "source": [
        "### **Training the DQN Agent**\n",
        "In this section, we train the DQN Agent to land the spacecraft in the Lunar Lander environment. We visualize the training process by plotting the learning curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb53-wMU6W5S"
      },
      "source": [
        "We setup the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bnye4aBV6W5T"
      },
      "outputs": [],
      "source": [
        "env_name = 'LunarLander-v3'\n",
        "env = gym.make(env_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3CXsPE86W5T"
      },
      "source": [
        "Next, we initialize our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pW36nGr6W5T"
      },
      "outputs": [],
      "source": [
        "dqn_agent = DQN_Agent(env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdyXbKe66W5T"
      },
      "source": [
        "Finally, we execute the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "g_dmNIOG6W5T",
        "outputId": "11fdd06c-5caf-4b84-f61f-7beda4d55f62"
      },
      "outputs": [],
      "source": [
        "dqn_agent.train(num_episodes=325)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rxd-klts6W5T"
      },
      "source": [
        "### **Visualizing the DQN agent's policy**\n",
        "Now that we have trained a DQN agent on the Lunar Lander task, we can visualize its behavior (i.e., policy)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL4CjoS46W5U"
      },
      "outputs": [],
      "source": [
        "# Create a and reset the environment, and set the random seed.\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "\n",
        "# Wrap the environment using renderlab to make a video later.\n",
        "env = renderlab.RenderFrame(env, \"./output\")\n",
        "\n",
        "# Visualize the agent.\n",
        "simulate_agent(env, dqn_agent.policy_net)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
